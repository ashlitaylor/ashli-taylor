---
title: Analyzing The Google Books Trigrams dataset with Pig on AWS
subtitle: Pig, AWS, S3, EC2, EMR
date: 2018-01-01T06:00:00.000+00:00
thumb_img_path: "/images/trigrams.jpg"
excerpt: 'Analyzing the Google Trigrams ~120GB dataset using the interactive PIG shell
  provided by EMR from the command line (grunt) using one master instance and 15 core
  instances. '
layout: post
content_img_path: ''
canonical_url: ''
github: <a href="https://github.com/ashlitaylor/Trigrams" target="_blank" class="btn
  btn-primary">GitHub Repository</a>

---
github: <a href="https://github.com/ashlitaylor/Trigrams" target="_blank" class="btn  btn-primary">GitHub Repository</a>

### Background

An ‘n-gram’ is a phrase with n words. The Google Trigrams dataset is a \~120GB dataset of phrases comprised of three words from Google Books. I tried out [apache pig](http://pig.apache.org/) to process this dataset on Amazon Web Services (AWS). The script I created outputs the 20 trigrams having the highest average number of appearances per book along with their corresponding averages in tab-separated format, sorted in descending order.

The services I used for this task are Amazon S3 storage, Amazon Elastic Cloud Computing (EC2) virtual servers in the cloud, and Amazon Elastic MapReduce (EMR) managed Hadoop framework. AWS allows use of up to 20 instances in total (that means 1 master instance and up to 19 core instances) without filling out a “limit request form”. I performed this task using the interactive PIG shell provided by EMR from the command line (grunt) using one master instance and 15 core instances. My results are output and stored in a TSV file that I exported to an output directory on S3. 

```
-- #Load the dataset 
-- NOTE: This data is stored in the  in the US East (N. Virginia) region. 
-- My machine cluster is set up in this region for computation to avoid incurring data transfer charges. 
trigrams = LOAD 's3://cse6242-2019spring-trigrams-big/*' AS (trigram:chararray, year: int, occurrences:int, books:int);

-- This line filters the entries with at least 400 occurrences and at least 15 books.
trigrams_filter = FILTER trigrams BY occurrences >= 400 AND books >= 15;

-- Consolidate the trigrams
trigrams_group = GROUP trigrams_filter BY trigram;

-- Equation: trigram average = no. occurences/no. books
trigrams_avg = FOREACH trigrams_group GENERATE $0 AS trigram, (double)SUM($1.occurrences)/(double)SUM($1.books) AS trigramAVG;

-- Order the trigrams by their averages, highest to lowest
trigrams_order = ORDER trigrams_avg BY trigramAVG DESC, trigram ASC;

-- Limiting to the top 20 trigram averages
trigrams_top = LIMIT trigrams_order 20;

-- Output the top 20 trigrams to a TSV file
STORE trigrams_top INTO 's3://largerun314/run1' USING PigStorage('\t');
```