---
title: Analyzing The Google Books Trigrams dataset with Pig on AWS
subtitle: Pig, AWS, S3, EC2, EMR
date: 2018-01-01T06:00:00.000+00:00
thumb_img_path: "/images/trigrams.jpg"
excerpt: 'Analyzing the Google Trigrams ~120GB dataset using the interactive PIG shell
  provided by EMR from the command line (grunt) using one master instance and 15 core
  instances. '
layout: post
content_img_path: ''
canonical_url: ''
github: <a href="https://github.com/ashlitaylor/Trigrams" target="_blank" class="btn
  btn-primary">GitHub Repository</a>

---
github: <a href="https://github.com/ashlitaylor/Trigrams" target="_blank" class="btn  btn-primary">GitHub Repository</a>

### Background

An ‘n-gram’ is a phrase with n words. The Google Trigrams dataset is a \~120GB dataset of phrases comprised of three words from Google Books. I tried out [apache pig](http://pig.apache.org/) to process this dataset on Amazon Web Services (AWS). The script I created outputs the 20 trigrams having the highest average number of appearances per book along with their corresponding averages in tab-separated format, sorted in descending order.

The services I used for this task are Amazon S3 storage, Amazon Elastic Cloud Computing (EC2) virtual servers in the cloud, and Amazon Elastic MapReduce (EMR) managed Hadoop framework. AWS allows use of up to 20 instances in total (that means 1 master instance and up to 19 core instances) without filling out a “limit request form”. I performed this task using the interactive PIG shell provided by EMR from the command line (grunt) using one master instance and 15 core instances. My results are output and stored in a TSV file that I exported to an output directory on S3. 